---
title: "Notebook_Clase5_y_6_Evaluacion_Modelos_Outliers_Colinealidad"
author: "Diego Figueroa"
date: "2025-06-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### El problema Fundamental en Ciencia de Datos para Modelos Predictivos

#### Trade-off entre Sesgo y Varianza

En modelos predictivos, buscamos minimizar el error total. Este error se puede descomponer en tres partes:

$$
\mathbb{E}\left[(\hat{f}(x) - f(x))^2\right] = \left(\text{Bias}[\hat{f}(x)]\right)^2 + \text{Var}[\hat{f}(x)] + \sigma^2
$$

- **Sesgo (Bias)**: Error por hacer suposiciones demasiado simples sobre los datos. Conduce a *underfitting*.
- **Varianza (Variance)**: Error por hacer el modelo muy sensible a las variaciones del conjunto de entrenamiento. Conduce a *overfitting*.
- **\( \sigma^2 \)**: Error irreducible debido al ruido en los datos.

---

#### Gr谩fico conceptual (para agregar en un informe)



```{r, echo=FALSE, out.width="80%", fig.align="center"}
knitr::include_graphics("C:/Users/diego/OneDrive/Escritorio/Diplomado Data Science/Diplomado PUCV/r_supervisedmodelling_pca_regression_2025/resources/bias_variance_tradeoff.png")

```

#### Ejemplos:
- **Modelo con alto sesgo**: Regresi贸n lineal simple sobre datos no lineales.
- **Modelo con alta varianza**: rbol de decisi贸n sin podar o regresi贸n polinomial de alto grado.

\newpage

### Problemas comunes en regresi贸n lineal m煤ltiple

---

#### 1. Outliers

##### Diagn贸stico:
- Visualizaci贸n con gr谩ficos de residuos.
- Distancia de Cook \( D_i \):
  
  $$
  D_i = \frac{(e_i^2)}{p \cdot \hat{\sigma}^2} \cdot \left( \frac{h_{ii}}{(1 - h_{ii})^2} \right)
  $$

##### Soluciones:
- Transformaciones (log, ra铆z cuadrada).
- Regresi贸n robusta (ej: `rlm()` en R).
- Winsorizaci贸n de extremos.

---

#### 2. Multicolinealidad

##### Diagn贸stico:
- VIF (Variance Inflation Factor):

  $$
  \text{VIF}_j = \frac{1}{1 - R_j^2}
  $$

- Si \( \text{VIF}_j > 5 \): hay evidencia de multicolinealidad.

##### Soluciones:
- Eliminar variables altamente correlacionadas.
- Transformar o combinar predictores (ej: PCA).
- Usar **Ridge Regression**:

  $$
  \min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}
  $$
### Ejemplo Simulado

```{r}
eps<-rnorm(90)
eps2<-rnorm(90)*0.01
x1<-rnorm(90)*2
x2<-0.5*x1+eps2
x3<-rnorm(90)
y=3*x1+8*x2+2*x3+3+eps
lin<-lm(y~x1+x2+x3)
summary(lin)
cor.test(x1,x2)

X<-matrix(1,90,4)
X[,2]<-x1
X[,3]<-x2
X[,4]<-x3
solve(t(X)%*%X)
```
\newpage

### Regularizaci贸n: LASSO, Ridge y Elastic Net

La regularizaci贸n es una t茅cnica que permite controlar la complejidad del modelo y evitar el sobreajuste (overfitting) penalizando los coeficientes de la regresi贸n. A continuaci贸n, se detallan tres m茅todos ampliamente utilizados:

---

#### Ridge Regression (Regresi贸n de cresta)

Penaliza la suma de los cuadrados de los coeficientes. La funci贸n objetivo a minimizar es:

$$
\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}
$$

Donde:
- \( \lambda \) es el par谩metro de penalizaci贸n,
- \( \beta_j \) son los coeficientes del modelo,
- \( y_i \) es la variable dependiente,
- \( \hat{y}_i \) es la predicci贸n del modelo.

Buscamos solucionar:
- La colinealidad de baja dimensi贸n (pocos predictores)

##### Caso Ejemplo: MTCARS

```{r}
mtcars2 <- within(mtcars, {
  vs <- factor(vs, labels = c("V", "S"))
  am <- factor(am, labels = c("automatic", "manual"))
  cyl <- factor(cyl, labels = c("4", "6","8"))
  gear <- factor(gear, labels = c("3", "4","5"))
  carb <- factor(carb, labels = c("1", "2","3","4","6","8"))
})
#Comentario:
#Aunque las variables Cyl, gear y carb tienen el sentido del orden, lo defino 
#como "factor" porque cilindros de un auto, no se puede comparar con cilindros de otra.

#Se puede incorporar "ordered" cuando estamos con variables que tienen espacio regular
#entre niveles (pocos niveles) tal como temperatura= 20, 40 y 60掳C, concentracion de
#producto 10, 20 y 30 g/L. En las salidas se entrega resultados con regresi贸n polinomial
lin<-lm(mpg~cyl+disp+hp+drat+wt+qsec+gear+carb,data=mtcars2)
summary(lin)

```
```{r}
cor(mtcars[,c(-2,-8-9-10,-11)])
```
Hay predictores que tienen alta correlaci贸n entre predictores como hp y wt

```{r}

library(car)
car::vif(lin)#calculo de los VIF para cada variable tomando en cuenta los grados de libertad de las variables categoricas
cal.vif<-car::vif(lin)
cal.vif[,3]<-cal.vif[,3]^2
cal.vif#hay que considerar el cuadrado para poder ocupar las mismas reglas arbitrarias

```

```{r}
###################################################
#1a soluci贸n: eliminar las variables problematicas#
###################################################

#eliminamos la variable con el VIF el m谩s alto
lin2<-lm(mpg~cyl+hp+drat+wt+qsec+gear+carb,data=mtcars2)
car::vif(lin2)
cal.vif2<-car::vif(lin2)
cal.vif2[,3]<-cal.vif2[,3]^2
cal.vif2
summary(lin2)
```

```{r}
#el VIF asociado a la variable "hp" sigue alto

lin3<-lm(mpg~cyl+drat+wt+qsec+gear+carb,data=mtcars2)
car::vif(lin3)
cal.vif3<-car::vif(lin3)
cal.vif3[,3]<-cal.vif3[,3]^2
cal.vif3
summary(lin3)
```
**Se puede eliminar otras variables de este modelo en una etapa posterior, pero en esta etapa hemos arreglado el problema de colinealidad**
**quizas seria un error eliminar "wt" porque es la variable la m谩s significativa y la mas correlacionada con mpg. Podemos parar aqu铆 o seguir eliminando "qsec"**

```{r}
lin4<-lm(mpg~cyl+drat+wt+gear+carb,data=mtcars2)
car::vif(lin4)
cal.vif4<-car::vif(lin4)
cal.vif4[,3]<-cal.vif4[,3]^2
cal.vif4
summary(lin4)#Ninguna variable parece pertinente en este modelo, salvo quiz谩s la variable "carb". El modelo 3 es mejor.

```
Cuando modifico el lambda estoy penalizando el modelo, mientras no lo haga hago la regresi贸n lineal com煤n y silvestre.
```{r}
library(MASS)
lm.ridge(mpg~cyl+disp+hp+drat+wt+qsec+gear+carb,data=mtcars2, lambda = 0)#NO ridge regression
h<-seq(14, 15, by=0.01)
rid<-lm.ridge(mpg~cyl+disp+hp+drat+wt+qsec+gear+carb,data=mtcars2, lambda = h)#ridge regression
#rid$GCV
#rid$coef[,48]
#Para ubicar exactamente el punto optimal y aplicarlo 
```

```{r}
plot(h,rid$GCV)#GCV Validaci贸n Cruzada Generalizada
```


```{r}
rid<-lm.ridge(mpg~cyl+disp+hp+drat+wt+qsec+gear+carb,data=mtcars2, lambda = 14.653)
rid$coef#estimaci贸n con el parametro "ridge" optimal


```







---

#### LASSO (Least Absolute Shrinkage and Selection Operator)

Penaliza la suma de los valores absolutos de los coeficientes, lo que puede llevar a que algunos coeficientes se reduzcan exactamente a cero (selecci贸n de variables):

$$
\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
$$
Buscamos solucionar:
- El problema de colinealidad de alta dimensi贸n (muchos predictores y observaciones)


---

#### Elastic Net

Es una combinaci贸n entre Ridge y LASSO, ponderada por un par谩metro de mezcla \( \alpha \in [0, 1] \):

$$
\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \left[ \alpha \sum_{j=1}^{p} |\beta_j| + (1 - \alpha) \sum_{j=1}^{p} \beta_j^2 \right] \right\}
$$

- Si \( \alpha = 1 \), se obtiene LASSO.
- Si \( \alpha = 0 \), se obtiene Ridge.

---

En R, estos modelos se pueden ajustar con la funci贸n `glmnet()` del paquete `glmnet`. Por ejemplo:

```r
library(glmnet)
glmnet(x, y, alpha = 0)   # Ridge
glmnet(x, y, alpha = 1)   # LASSO
glmnet(x, y, alpha = 0.5) # Elastic Net
```

\newpage

### 驴Qu茅 hacer cuando la regularizaci贸n no soluciona la Colinealidad?

En ciencia de datos, cuando enfrentamos **alta multicolinealidad** o un n煤mero elevado de predictores, podemos utilizar modelos que combinan regresi贸n con **reducci贸n de dimensionalidad**, como:

- **PCR** (Principal Component Regression)
- **PLS** (Partial Least Squares)

**驴Por qu茅 no usar directamente LASSO o Ridge?**

LASSO y Ridge buscan reducir el sobreajuste penalizando los coeficientes del modelo (regularizaci贸n), pero:

- No transforman las variables predictoras.

- Si hay multicolinealidad fuerte, no eliminan completamente el problema, aunque lo mitigan.

Aqu铆 es donde PCR y PLS entran como estrategias basadas en transformaci贸n de los datos.


#### 1. Principal Component Regression (PCR)

####  Idea

PCR aplica primero An谩lisis de Componentes Principales (PCA) sobre los predictores \( \mathbf{X} \), y luego utiliza los primeros \( k \) componentes para ajustar una regresi贸n lineal sobre la variable respuesta \( y \).PCR no considera la variable respuesta $y$ al crear los componentes. Es decir, puede elegir componentes que explican mucha varianza en $X$, pero no necesariamente son relevantes para predecir $y$

#### М Ecuaci贸n

Sea:

- \( \mathbf{X} \): matriz de predictores
- \( \mathbf{W} \): vectores propios de PCA
- \( \mathbf{Z} = \mathbf{XW} \): componentes principales

Entonces el modelo es:

$$
y = \mathbf{Z} \beta + \epsilon
$$

> **Importante**: los componentes son seleccionados solo para maximizar la varianza de \( \mathbf{X} \), no su relaci贸n con \( y \).

Presenta problemas al hacer PCA, ya que pierdo poder predictivo (incremento el sesgo) al solo tomar los k componentes, ya que los n-k componentes restantes, siendo n el total de componentes principales generados, no est谩n siendo incluidos y que s铆 aportan informaci贸n predictiva. Pierdo bondad de ajuste.PCA elige los componentes que explican la mayor varianza en los regresores $X$, sin tener en cuenta si esos componentes est谩n relacionados con la variable respuesta $Y$. 

---

\newpage


#### 2. Partial Least Squares Regression (PLS)

#####  Idea

PLS tambi茅n reduce la dimensionalidad, pero a diferencia de PCR, busca componentes que **maximizan simult谩neamente** la varianza en \( \mathbf{X} \) y la covarianza con \( y \).

##### М Ecuaci贸n

El primer componente se define como:

$$
T_1 = \mathbf{X} w_1 \quad \text{con } w_1 = \arg\max \text{Cov}(\mathbf{X}w, y)^2
$$

Luego se ajusta:

$$
y = T \beta + \epsilon
$$

donde \( T = \mathbf{XW} \).

> **Ventaja**: al considerar \( y \) en la creaci贸n de los componentes, suele tener mejor desempe帽o predictivo que PCR.

---

###  Comparaci贸n

\[
\begin{array}{|l|c|c|}
\hline
\textbf{Caracter铆stica} & \textbf{PCR} & \textbf{PLS} \\
\hline
Usa \ y \ para crear componentes & \text{No} & \text{S铆} \\
\hline
Reduce dimensionalidad & \text{S铆 (PCA)} & \text{S铆 (Covarianza)} \\
\hline
Manejo de multicolinealidad & \text{S铆} & \text{Mejor} \\
\hline
Enfoque predictivo & \text{Moderado} & \text{Alto} \\
\hline
Interpretabilidad & \text{Baja} & \text{Media} \\
\hline
\end{array}
\]

---


###  Cu谩ndo usar cada uno

- Multicolinealidad severa: **PLS** o **PCR**
- Muchas variables predictoras: **LASSO**, **PLS**
- Predicci贸n pura: **PLS**
- Interpretabilidad: **Ridge**, **LASSO**




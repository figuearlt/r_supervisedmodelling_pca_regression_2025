---
title: "Notebook_Clase5_y_6_Evaluacion_Modelos_Outliers_Colinealidad"
author: "Diego Figueroa"
date: "2025-06-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### El problema Fundamental en Ciencia de Datos para Modelos Predictivos

#### Trade-off entre Sesgo y Varianza

En modelos predictivos, buscamos minimizar el error total. Este error se puede descomponer en tres partes:

$$
\mathbb{E}\left[(\hat{f}(x) - f(x))^2\right] = \left(\text{Bias}[\hat{f}(x)]\right)^2 + \text{Var}[\hat{f}(x)] + \sigma^2
$$

- **Sesgo (Bias)**: Error por hacer suposiciones demasiado simples sobre los datos. Conduce a *underfitting*.
- **Varianza (Variance)**: Error por hacer el modelo muy sensible a las variaciones del conjunto de entrenamiento. Conduce a *overfitting*.
- **\( \sigma^2 \)**: Error irreducible debido al ruido en los datos.

---

#### Gr谩fico conceptual (para agregar en un informe)



```{r, echo=FALSE, out.width="80%", fig.align="center"}
knitr::include_graphics("C:/Users/diego/OneDrive/Escritorio/Diplomado Data Science/Diplomado PUCV/clase1_r_supervised_pca_regression_2025/resources/bias_variance_tradeoff.png")

```

#### Ejemplos:
- **Modelo con alto sesgo**: Regresi贸n lineal simple sobre datos no lineales.
- **Modelo con alta varianza**: rbol de decisi贸n sin podar o regresi贸n polinomial de alto grado.

\newpage

### Problemas comunes en regresi贸n lineal m煤ltiple

---

#### 1. Outliers

##### Diagn贸stico:
- Visualizaci贸n con gr谩ficos de residuos.
- Distancia de Cook \( D_i \):
  
  $$
  D_i = \frac{(e_i^2)}{p \cdot \hat{\sigma}^2} \cdot \left( \frac{h_{ii}}{(1 - h_{ii})^2} \right)
  $$

##### Soluciones:
- Transformaciones (log, ra铆z cuadrada).
- Regresi贸n robusta (ej: `rlm()` en R).
- Winsorizaci贸n de extremos.

---

#### 2. Multicolinealidad

##### Diagn贸stico:
- VIF (Variance Inflation Factor):

  $$
  \text{VIF}_j = \frac{1}{1 - R_j^2}
  $$

- Si \( \text{VIF}_j > 5 \): hay evidencia de multicolinealidad.

##### Soluciones:
- Eliminar variables altamente correlacionadas.
- Transformar o combinar predictores (ej: PCA).
- Usar **Ridge Regression**:

  $$
  \min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}
  $$
### Ejemplo Simulado

```{r}
eps<-rnorm(90)
eps2<-rnorm(90)*0.01
x1<-rnorm(90)*2
x2<-0.5*x1+eps2
x3<-rnorm(90)
y=3*x1+8*x2+2*x3+3+eps
lin<-lm(y~x1+x2+x3)
summary(lin)
cor.test(x1,x2)

X<-matrix(1,90,4)
X[,2]<-x1
X[,3]<-x2
X[,4]<-x3
solve(t(X)%*%X)
```
\newpage

### Regularizaci贸n: LASSO, Ridge y Elastic Net

La regularizaci贸n es una t茅cnica que permite controlar la complejidad del modelo y evitar el sobreajuste (overfitting) penalizando los coeficientes de la regresi贸n. A continuaci贸n, se detallan tres m茅todos ampliamente utilizados:

---

#### Ridge Regression (Regresi贸n de cresta)

Penaliza la suma de los cuadrados de los coeficientes. La funci贸n objetivo a minimizar es:

$$
\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}
$$

Donde:
- \( \lambda \) es el par谩metro de penalizaci贸n,
- \( \beta_j \) son los coeficientes del modelo,
- \( y_i \) es la variable dependiente,
- \( \hat{y}_i \) es la predicci贸n del modelo.

Buscamos solucionar:
- La colinealidad de baja dimensi贸n (pocos predictores)

---

#### LASSO (Least Absolute Shrinkage and Selection Operator)

Penaliza la suma de los valores absolutos de los coeficientes, lo que puede llevar a que algunos coeficientes se reduzcan exactamente a cero (selecci贸n de variables):

$$
\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
$$
Buscamos solucionar:
- El problema de colinealidad de alta dimensi贸n (muchos predictores y observaciones)


---

#### Elastic Net

Es una combinaci贸n entre Ridge y LASSO, ponderada por un par谩metro de mezcla \( \alpha \in [0, 1] \):

$$
\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \left[ \alpha \sum_{j=1}^{p} |\beta_j| + (1 - \alpha) \sum_{j=1}^{p} \beta_j^2 \right] \right\}
$$

- Si \( \alpha = 1 \), se obtiene LASSO.
- Si \( \alpha = 0 \), se obtiene Ridge.

---

En R, estos modelos se pueden ajustar con la funci贸n `glmnet()` del paquete `glmnet`. Por ejemplo:

```r
library(glmnet)
glmnet(x, y, alpha = 0)   # Ridge
glmnet(x, y, alpha = 1)   # LASSO
glmnet(x, y, alpha = 0.5) # Elastic Net
```

\newpage

### 驴Qu茅 hacer cuando la regularizaci贸n no soluciona la Colinealidad?

En ciencia de datos, cuando enfrentamos **alta multicolinealidad** o un n煤mero elevado de predictores, podemos utilizar modelos que combinan regresi贸n con **reducci贸n de dimensionalidad**, como:

- **PCR** (Principal Component Regression)
- **PLS** (Partial Least Squares)

**驴Por qu茅 no usar directamente LASSO o Ridge?**

LASSO y Ridge buscan reducir el sobreajuste penalizando los coeficientes del modelo (regularizaci贸n), pero:

- No transforman las variables predictoras.

- Si hay multicolinealidad fuerte, no eliminan completamente el problema, aunque lo mitigan.

Aqu铆 es donde PCR y PLS entran como estrategias basadas en transformaci贸n de los datos.


#### 1. Principal Component Regression (PCR)

####  Idea

PCR aplica primero An谩lisis de Componentes Principales (PCA) sobre los predictores \( \mathbf{X} \), y luego utiliza los primeros \( k \) componentes para ajustar una regresi贸n lineal sobre la variable respuesta \( y \).PCR no considera la variable respuesta $y$ al crear los componentes. Es decir, puede elegir componentes que explican mucha varianza en $X$, pero no necesariamente son relevantes para predecir $y$

#### М Ecuaci贸n

Sea:

- \( \mathbf{X} \): matriz de predictores
- \( \mathbf{W} \): vectores propios de PCA
- \( \mathbf{Z} = \mathbf{XW} \): componentes principales

Entonces el modelo es:

$$
y = \mathbf{Z} \beta + \epsilon
$$

> **Importante**: los componentes son seleccionados solo para maximizar la varianza de \( \mathbf{X} \), no su relaci贸n con \( y \).

Presenta problemas al hacer PCA, ya que pierdo poder predictivo (incremento el sesgo) al solo tomar los k componentes, ya que los n-k componentes restantes, siendo n el total de componentes principales generados, no est谩n siendo incluidos y que s铆 aportan informaci贸n predictiva. Pierdo bondad de ajuste.PCA elige los componentes que explican la mayor varianza en los regresores $X$, sin tener en cuenta si esos componentes est谩n relacionados con la variable respuesta $Y$. 

---

\newpage


#### 2. Partial Least Squares Regression (PLS)

#####  Idea

PLS tambi茅n reduce la dimensionalidad, pero a diferencia de PCR, busca componentes que **maximizan simult谩neamente** la varianza en \( \mathbf{X} \) y la covarianza con \( y \).

##### М Ecuaci贸n

El primer componente se define como:

$$
T_1 = \mathbf{X} w_1 \quad \text{con } w_1 = \arg\max \text{Cov}(\mathbf{X}w, y)^2
$$

Luego se ajusta:

$$
y = T \beta + \epsilon
$$

donde \( T = \mathbf{XW} \).

> **Ventaja**: al considerar \( y \) en la creaci贸n de los componentes, suele tener mejor desempe帽o predictivo que PCR.

---

###  Comparaci贸n

\[
\begin{array}{|l|c|c|}
\hline
\textbf{Caracter铆stica} & \textbf{PCR} & \textbf{PLS} \\
\hline
Usa \ y \ para crear componentes & \text{No} & \text{S铆} \\
\hline
Reduce dimensionalidad & \text{S铆 (PCA)} & \text{S铆 (Covarianza)} \\
\hline
Manejo de multicolinealidad & \text{S铆} & \text{Mejor} \\
\hline
Enfoque predictivo & \text{Moderado} & \text{Alto} \\
\hline
Interpretabilidad & \text{Baja} & \text{Media} \\
\hline
\end{array}
\]

---


###  Cu谩ndo usar cada uno

- Multicolinealidad severa: **PLS** o **PCR**
- Muchas variables predictoras: **LASSO**, **PLS**
- Predicci贸n pura: **PLS**
- Interpretabilidad: **Ridge**, **LASSO**



